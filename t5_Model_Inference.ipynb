{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5WhWe8aonFm",
        "outputId": "6f273e37-a00a-41e3-b864-321baea2ecc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shared.weight (32128, 768)\n",
            "encoder.embed_tokens.weight (32128, 768)\n",
            "encoder.block.0.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.0.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.0.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.0.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 12)\n",
            "encoder.block.0.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.0.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.0.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.0.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.1.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.1.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.1.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.1.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.1.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.1.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.1.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.1.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.2.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.2.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.2.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.2.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.2.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.2.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.2.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.2.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.3.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.3.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.3.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.3.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.3.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.3.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.3.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.3.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.4.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.4.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.4.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.4.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.4.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.4.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.4.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.4.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.5.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.5.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.5.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.5.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.5.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.5.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.5.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.5.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.6.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.6.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.6.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.6.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.6.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.6.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.6.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.6.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.7.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.7.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.7.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.7.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.7.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.7.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.7.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.7.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.8.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.8.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.8.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.8.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.8.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.8.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.8.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.8.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.8.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.9.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.9.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.9.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.9.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.9.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.9.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.9.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.9.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.9.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.10.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.10.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.10.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.10.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.10.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.10.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.10.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.10.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.10.layer.1.layer_norm.weight (768,)\n",
            "encoder.block.11.layer.0.SelfAttention.q.weight (768, 768)\n",
            "encoder.block.11.layer.0.SelfAttention.k.weight (768, 768)\n",
            "encoder.block.11.layer.0.SelfAttention.v.weight (768, 768)\n",
            "encoder.block.11.layer.0.SelfAttention.o.weight (768, 768)\n",
            "encoder.block.11.layer.0.layer_norm.weight (768,)\n",
            "encoder.block.11.layer.1.DenseReluDense.wi_0.weight (2048, 768)\n",
            "encoder.block.11.layer.1.DenseReluDense.wi_1.weight (2048, 768)\n",
            "encoder.block.11.layer.1.DenseReluDense.wo.weight (768, 2048)\n",
            "encoder.block.11.layer.1.layer_norm.weight (768,)\n",
            "encoder.final_layer_norm.weight (768,)\n",
            "decoder.embed_tokens.weight (32128, 768)\n",
            "decoder.block.0.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.0.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.0.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.0.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 12)\n",
            "decoder.block.0.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.0.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.0.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.0.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.0.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.0.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.0.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.0.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.0.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.1.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.1.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.1.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.1.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.1.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.1.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.1.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.1.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.1.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.1.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.1.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.1.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.1.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.2.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.2.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.2.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.2.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.2.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.2.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.2.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.2.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.2.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.2.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.2.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.2.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.2.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.3.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.3.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.3.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.3.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.3.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.3.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.3.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.3.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.3.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.3.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.3.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.3.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.3.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.4.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.4.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.4.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.4.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.4.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.4.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.4.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.4.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.4.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.4.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.4.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.4.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.4.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.5.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.5.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.5.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.5.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.5.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.5.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.5.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.5.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.5.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.5.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.5.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.5.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.5.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.6.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.6.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.6.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.6.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.6.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.6.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.6.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.6.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.6.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.6.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.6.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.6.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.6.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.7.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.7.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.7.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.7.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.7.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.7.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.7.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.7.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.7.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.7.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.7.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.7.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.7.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.8.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.8.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.8.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.8.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.8.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.8.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.8.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.8.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.8.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.8.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.8.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.8.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.8.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.8.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.9.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.9.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.9.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.9.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.9.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.9.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.9.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.9.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.9.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.9.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.9.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.9.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.9.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.9.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.10.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.10.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.10.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.10.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.10.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.10.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.10.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.10.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.10.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.10.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.10.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.10.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.10.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.10.layer.2.layer_norm.weight (768,)\n",
            "decoder.block.11.layer.0.SelfAttention.q.weight (768, 768)\n",
            "decoder.block.11.layer.0.SelfAttention.k.weight (768, 768)\n",
            "decoder.block.11.layer.0.SelfAttention.v.weight (768, 768)\n",
            "decoder.block.11.layer.0.SelfAttention.o.weight (768, 768)\n",
            "decoder.block.11.layer.0.layer_norm.weight (768,)\n",
            "decoder.block.11.layer.1.EncDecAttention.q.weight (768, 768)\n",
            "decoder.block.11.layer.1.EncDecAttention.k.weight (768, 768)\n",
            "decoder.block.11.layer.1.EncDecAttention.v.weight (768, 768)\n",
            "decoder.block.11.layer.1.EncDecAttention.o.weight (768, 768)\n",
            "decoder.block.11.layer.1.layer_norm.weight (768,)\n",
            "decoder.block.11.layer.2.DenseReluDense.wi_0.weight (2048, 768)\n",
            "decoder.block.11.layer.2.DenseReluDense.wi_1.weight (2048, 768)\n",
            "decoder.block.11.layer.2.DenseReluDense.wo.weight (768, 2048)\n",
            "decoder.block.11.layer.2.layer_norm.weight (768,)\n",
            "decoder.final_layer_norm.weight (768,)\n",
            "lm_head.weight (32128, 768)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Get all named parameters (weights + biases)\n",
        "state_dict = model.state_dict()\n",
        "\n",
        "# Convert all to NumPy\n",
        "weights_numpy = {name: param.detach().cpu().numpy() for name, param in state_dict.items()}\n",
        "\n",
        "# Optional: Print all weight names\n",
        "for name in weights_numpy:\n",
        "    print(name, weights_numpy[name].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "AS2gd2ViqCxF"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Translate English to French: Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"]  # shape: (1, seq_len)\n",
        "\n",
        "\n",
        "print(input_ids.shape)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHuEjE5SqDRq",
        "outputId": "a211457c-3987-49c0-9f0a-89348889ca94"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 13])\n",
            "tensor([[30355,    15,  1566,    12,  2379,    10,  8774,     6,   149,    33,\n",
            "            25,    58,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Ql-g5QcsBVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = weights_numpy[\"shared.weight\"]  # (vocab_size, d_model)  shared.weight->shared embedding matrix\n",
        "x_embed = embedding_matrix[input_ids.numpy()]      # shape: (1, seq_len, d_model)\n",
        "\n",
        "print(embedding_matrix, embedding_matrix.shape)\n",
        "print(\" \")\n",
        "print(x_embed, x_embed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNXkMOlWsA96",
        "outputId": "600ef5f5-0571-4c84-ce1b-df1872b20ed0"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.08053398e-01  1.27280390e+00 -1.33916453e-01 ... -8.01376700e-01\n",
            "   3.92395234e+00  9.71753988e-03]\n",
            " [ 1.58270750e+01  7.19118977e+00  1.51406145e+01 ...  5.45081472e+00\n",
            "  -2.59278812e+01  1.14962749e+01]\n",
            " [-1.84467244e+00 -8.89065981e-01 -1.29316864e+01 ...  4.10892248e+00\n",
            "   5.05712175e+00 -3.76946950e+00]\n",
            " ...\n",
            " [ 5.66406250e-01 -1.35937500e+00 -1.19531250e+00 ...  1.46484375e-01\n",
            "  -5.07812500e-01 -1.21093750e+00]\n",
            " [-8.90625000e-01  8.16406250e-01  6.36718750e-01 ... -1.20117188e-01\n",
            "   9.49218750e-01 -1.59375000e+00]\n",
            " [-8.32031250e-01  1.82812500e+00  1.37695312e-01 ... -2.10937500e-01\n",
            "  -1.18750000e+00 -5.11718750e-01]] (32128, 768)\n",
            " \n",
            "[[[ 6.88593197e+00  2.83577561e+00  4.36377525e+00 ...  1.21415033e+01\n",
            "   -3.79337239e+00  4.87758589e+00]\n",
            "  [-6.19857572e-04  5.20903826e+00  6.24792004e+00 ... -3.83192611e+00\n",
            "    1.55317676e+00  7.09706831e+00]\n",
            "  [ 1.99395204e+00 -2.70621324e+00  2.15984192e+01 ...  8.31336975e+00\n",
            "    4.03384495e+00 -4.74461937e+00]\n",
            "  ...\n",
            "  [-3.58882689e+00  6.64033175e+00  3.26323175e+00 ... -7.23639727e+00\n",
            "   -1.58176243e-01 -1.23220272e+01]\n",
            "  [-1.74790025e-02 -1.93775594e+00 -2.86076403e+00 ...  2.88028985e-01\n",
            "    7.34068304e-02 -9.43829632e+00]\n",
            "  [ 1.58270750e+01  7.19118977e+00  1.51406145e+01 ...  5.45081472e+00\n",
            "   -2.59278812e+01  1.14962749e+01]]] (1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nk54z8Fjsjs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Block — Self-Attention (Layer 0)"
      ],
      "metadata": {
        "id": "aY2sRF_rtQH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#encoder layer weights\n",
        "layer = model.encoder.block[0]\n",
        "ln1_weight = layer.layer[0].layer_norm.weight.detach().numpy()  # (768,)\n",
        "ln2_weight = layer.layer[1].layer_norm.weight.detach().numpy()  # (768,)"
      ],
      "metadata": {
        "id": "WOqwVvxwEn9s"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LayerNorm before attention (incase of t5)\n",
        "def layer_norm(x, weight):\n",
        "    mean = x.mean(-1, keepdims=True)\n",
        "    var = x.var(-1, keepdims=True)\n",
        "    return (x - mean) / np.sqrt(var + 1e-6) * weight\n",
        "x_norm = layer_norm(x_embed, ln1_weight)   #ln1_weight->LayerNorm weight before attention\n",
        "print(\"After LayerNorm (attn):\", x_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAaKP3oqG_tO",
        "outputId": "bed92768-9aed-4bf4-a956-e81b48d500e3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After LayerNorm (attn): (1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight_keys = [\n",
        "    (\"q_w\", \"encoder.block.0.layer.0.SelfAttention.q.weight\"),\n",
        "    (\"k_w\", \"encoder.block.0.layer.0.SelfAttention.k.weight\"),\n",
        "    (\"v_w\", \"encoder.block.0.layer.0.SelfAttention.v.weight\"),\n",
        "    (\"o_w\", \"encoder.block.0.layer.0.SelfAttention.o.weight\")\n",
        "]\n",
        "\n",
        "for name, key in weight_keys:\n",
        "    weight = weights_numpy[key]\n",
        "    print(f\"{name} ({key}): shape = {weight.shape}\")\n",
        "    print(weight, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RSorsL6sjpd",
        "outputId": "776d9eef-4ae1-46d7-d8b1-8056bb299715"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_w (encoder.block.0.layer.0.SelfAttention.q.weight): shape = (768, 768)\n",
            "[[-0.01452527  0.13774897  0.00905681 ... -0.05845838 -0.03574539\n",
            "  -0.01240443]\n",
            " [ 0.06178693  0.01419278 -0.01175551 ... -0.06018291 -0.05362125\n",
            "  -0.01797134]\n",
            " [ 0.00519627 -0.0433369  -0.05081028 ...  0.07576472 -0.04193865\n",
            "   0.09032436]\n",
            " ...\n",
            " [-0.0037946   0.00894625  0.03839661 ...  0.04620769  0.00765166\n",
            "  -0.00443411]\n",
            " [-0.02524992 -0.03872993  0.00699381 ...  0.04815986  0.02321322\n",
            "  -0.03827596]\n",
            " [ 0.00546945 -0.00330857 -0.0223488  ...  0.02904797 -0.03176966\n",
            "  -0.01060294]] \n",
            "\n",
            "k_w (encoder.block.0.layer.0.SelfAttention.k.weight): shape = (768, 768)\n",
            "[[ 7.34305196e-03  6.61422253e-01  2.25334123e-01 ... -6.03573620e-01\n",
            "  -2.29642674e-01 -2.23470833e-02]\n",
            " [ 3.04094225e-01 -2.41966367e-01  2.33838186e-02 ... -5.28845072e-01\n",
            "   3.73502541e-03 -5.75851500e-01]\n",
            " [ 5.12275934e-01 -3.55967283e-01 -6.63109601e-01 ...  1.66478440e-01\n",
            "  -9.13110852e-01  9.27042961e-01]\n",
            " ...\n",
            " [ 3.80679280e-01  2.78371155e-01  1.00625763e-02 ...  1.15573876e-01\n",
            "   1.21549577e-01 -2.95718700e-01]\n",
            " [ 7.43699202e-04 -4.98372555e-01  4.37100619e-01 ...  2.38509662e-02\n",
            "  -4.16062802e-01 -7.31618004e-03]\n",
            " [ 1.58342212e-01 -8.99664611e-02 -4.21525151e-01 ... -6.18629083e-02\n",
            "   2.29334801e-01  1.51363760e-01]] \n",
            "\n",
            "v_w (encoder.block.0.layer.0.SelfAttention.v.weight): shape = (768, 768)\n",
            "[[ 0.42156917  0.05874773 -0.43245387 ... -0.09829867  0.29593962\n",
            "  -0.14172284]\n",
            " [-0.6478909  -0.01644575  0.16434926 ...  0.00885074  0.19724232\n",
            "   0.02210759]\n",
            " [-0.26628503  0.00112589  0.17008613 ... -0.23086552 -0.32168043\n",
            "   0.02141949]\n",
            " ...\n",
            " [ 0.12576298 -0.01308218 -0.26912472 ...  0.22732802 -0.01293854\n",
            "  -0.00827893]\n",
            " [ 0.05326521 -0.02319695 -0.05669171 ...  0.10280143  0.1887119\n",
            "   0.14996232]\n",
            " [ 0.0017735   0.02440236 -0.1612454  ... -0.07450264 -0.01707762\n",
            "   0.1965105 ]] \n",
            "\n",
            "o_w (encoder.block.0.layer.0.SelfAttention.o.weight): shape = (768, 768)\n",
            "[[-0.18377799  0.44402224  0.10947024 ... -0.01297785 -0.14451618\n",
            "  -0.05323481]\n",
            " [-0.13703763 -0.00205273  0.25510684 ...  0.00181597 -0.11007664\n",
            "  -0.04666603]\n",
            " [ 0.06680252 -0.11922307 -0.0574021  ... -0.25342727 -0.21813107\n",
            "  -0.05141294]\n",
            " ...\n",
            " [-0.04798868 -0.25380751  0.06711552 ... -0.33454892 -0.0184827\n",
            "   0.45510438]\n",
            " [-0.13629194 -0.19542655  0.11648705 ... -0.15781167  0.33716017\n",
            "   0.21117656]\n",
            " [-0.06298815 -0.04569861 -0.30883044 ...  0.3408082   0.1826255\n",
            "   0.42772734]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_w = weights_numpy[\"encoder.block.0.layer.0.SelfAttention.q.weight\"]\n",
        "k_w = weights_numpy[\"encoder.block.0.layer.0.SelfAttention.k.weight\"]\n",
        "v_w = weights_numpy[\"encoder.block.0.layer.0.SelfAttention.v.weight\"]\n",
        "o_w = weights_numpy[\"encoder.block.0.layer.0.SelfAttention.o.weight\"]\n"
      ],
      "metadata": {
        "id": "vyQ2cBn5t1eH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q_w.shape)   # (d_model, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT7B9W_JzeFy",
        "outputId": "178a48fd-6e96-4d51-ae42-a78c781bc011"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(768, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating q,k and v"
      ],
      "metadata": {
        "id": "d1v35yuPttmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = np.matmul(x_norm, q_w.T)  # (1, seq_len, d_model)\n",
        "k = np.matmul(x_norm, k_w.T)\n",
        "v = np.matmul(x_norm, v_w.T)"
      ],
      "metadata": {
        "id": "6RMCITeCtwuc"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "133FoMPOzhMf",
        "outputId": "a863fb8c-9a51-419e-ec78-d81fe6ae2fd0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting into heads"
      ],
      "metadata": {
        "id": "gUEDFqcNud3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into heads: (1, seq_len, num_heads, head_dim) → transpose → (1, num_heads, seq_len, head_dim)\n",
        "def split_heads(x, num_heads=12):\n",
        "    batch, seq_len, d_model = x.shape\n",
        "    return x.reshape(batch, seq_len, num_heads, d_model // num_heads).transpose(0, 2, 1, 3)\n",
        "\n",
        "n_heads = model.config.num_heads\n",
        "d_model = model.config.d_model\n",
        "print(n_heads,d_model)\n",
        "qh, kh, vh = split_heads(q), split_heads(k), split_heads(v)  # shape: (1, 8, seq_len, 64)\n",
        "print(qh)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdTudgv1ugle",
        "outputId": "f0b59d4a-3fdb-47d5-94ab-2b6d1bae4392"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12 768\n",
            "[[[[-3.81109118e-01 -1.53292790e-01  6.01664893e-02 ... -8.16766769e-02\n",
            "    -2.12847963e-01  1.95742786e-01]\n",
            "   [ 1.18703805e-02  2.89509892e-01 -1.27281249e-01 ...  1.54057488e-01\n",
            "    -9.44998711e-02  5.56298308e-02]\n",
            "   [ 1.91411395e-02 -5.63554354e-02  2.01860726e-01 ... -1.39355078e-01\n",
            "    -2.29740977e-01 -1.97009236e-01]\n",
            "   ...\n",
            "   [-1.99008837e-01  2.38384664e-01 -2.11748570e-01 ...  1.18108671e-02\n",
            "    -2.47325543e-02  1.80457339e-01]\n",
            "   [-1.73620343e-01  1.13829672e-01 -1.54227585e-01 ... -3.55760306e-02\n",
            "    -6.77405968e-02  1.35670789e-02]\n",
            "   [ 5.31724393e-01  3.33801389e-01 -2.88328350e-01 ...  1.35146141e-01\n",
            "     2.12402388e-01  2.46586755e-01]]\n",
            "\n",
            "  [[ 3.44557762e-02  1.12452686e-01 -1.57856569e-03 ...  1.27684832e-01\n",
            "     1.11308917e-01 -2.46218406e-02]\n",
            "   [-6.60367981e-02  1.30099639e-01  1.96978062e-01 ... -7.55843520e-02\n",
            "     9.16574448e-02  4.99827787e-04]\n",
            "   [ 1.12402678e-01  2.75640581e-02 -3.37384641e-04 ...  8.61378461e-02\n",
            "     1.43078789e-01 -1.16522662e-01]\n",
            "   ...\n",
            "   [-4.45829630e-02 -6.40418231e-02  2.94006523e-02 ...  3.40780802e-02\n",
            "     1.12986624e-01 -1.35461345e-01]\n",
            "   [ 1.91235095e-02  2.80472860e-02 -6.60385750e-03 ... -4.66646403e-02\n",
            "     6.75239861e-02 -6.31742179e-02]\n",
            "   [ 5.08279204e-01 -2.06285402e-01  3.83740366e-02 ...  4.52877998e-01\n",
            "     9.08993185e-02 -1.36421278e-01]]\n",
            "\n",
            "  [[-1.13996834e-01  6.83015287e-02  8.24250057e-02 ...  1.27573878e-01\n",
            "    -1.28269657e-01 -1.84979245e-01]\n",
            "   [ 6.48236722e-02 -2.26203334e-02 -1.01701483e-01 ...  2.33636554e-02\n",
            "    -6.14311770e-02  8.09494406e-02]\n",
            "   [ 6.12982213e-02  1.39109969e-01  1.44023448e-03 ...  3.91573794e-02\n",
            "    -1.01074815e-01 -8.79072398e-03]\n",
            "   ...\n",
            "   [-7.89305940e-02 -1.12919502e-01  2.29702458e-01 ...  1.34097621e-01\n",
            "     9.65683535e-02 -6.85678124e-02]\n",
            "   [ 8.59628543e-02  2.35042050e-01  6.74766898e-02 ...  8.34692419e-02\n",
            "     1.16566621e-01  1.22207560e-01]\n",
            "   [-6.95474148e-02 -1.84026435e-01 -5.63703477e-02 ... -7.66901225e-02\n",
            "     1.43029261e-02 -1.07977435e-01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-9.47015136e-02  9.10520405e-02 -6.14695996e-02 ... -2.41642147e-02\n",
            "    -6.30256347e-03  1.56671554e-02]\n",
            "   [ 4.24761884e-02  8.55185091e-03 -1.32931359e-02 ... -2.10533813e-02\n",
            "    -4.63435948e-02  3.64768244e-02]\n",
            "   [ 1.89691097e-01  2.85248011e-02 -5.81238829e-02 ... -2.00546205e-01\n",
            "    -8.60095918e-02  1.54874370e-01]\n",
            "   ...\n",
            "   [ 1.85888894e-02  6.04518130e-02  7.37718269e-02 ... -3.02991010e-02\n",
            "     1.47655889e-01  1.14967786e-02]\n",
            "   [-1.04930766e-01 -4.08849455e-02  5.31926751e-04 ... -4.75382581e-02\n",
            "     5.84501810e-02  9.58486721e-02]\n",
            "   [-1.36847887e-02  1.73423253e-02 -5.64562231e-02 ... -5.19510433e-02\n",
            "     6.15439117e-02 -7.74796084e-02]]\n",
            "\n",
            "  [[ 4.51074913e-02 -2.08840482e-02  5.30457199e-02 ...  3.23423743e-02\n",
            "     1.40139699e-01  6.95848316e-02]\n",
            "   [ 2.15389617e-02 -9.27314907e-03  1.64196491e-02 ... -6.42604008e-03\n",
            "     2.76520848e-03 -6.09104373e-02]\n",
            "   [ 4.14056964e-02 -3.48121114e-02  1.14394233e-01 ...  1.20247349e-01\n",
            "    -7.30002671e-03  1.38640583e-01]\n",
            "   ...\n",
            "   [ 5.70957214e-02 -8.49767849e-02 -4.38858680e-02 ...  3.76786068e-02\n",
            "     4.43804860e-02  1.30318105e-03]\n",
            "   [ 1.62492663e-01  3.77177149e-02  3.98144312e-02 ...  2.92157233e-02\n",
            "     1.53871626e-01  5.14358133e-02]\n",
            "   [-7.39729404e-02 -5.62936738e-02  3.08375601e-02 ... -1.03423707e-01\n",
            "    -4.77935821e-02  5.55959791e-02]]\n",
            "\n",
            "  [[ 9.54339430e-02 -1.04730360e-01 -7.11924955e-02 ...  2.94573046e-02\n",
            "     1.06711522e-01  8.08087438e-02]\n",
            "   [-6.04154542e-02 -1.07944235e-02 -5.53143024e-02 ...  4.30207625e-02\n",
            "    -3.32019888e-02  9.48358029e-02]\n",
            "   [ 2.80481018e-02  5.06008826e-02 -8.23452920e-02 ...  1.94638923e-01\n",
            "     1.21268436e-01  5.17106056e-03]\n",
            "   ...\n",
            "   [-1.78828716e-01 -5.32057211e-02  6.12528622e-03 ... -2.26900168e-03\n",
            "     2.19035354e-02  1.22275781e-02]\n",
            "   [-3.19504663e-02 -9.86207500e-02 -1.17385536e-01 ... -2.99604610e-03\n",
            "    -4.45887856e-02  1.94862530e-01]\n",
            "   [-2.46818475e-02  1.84170082e-01 -9.76949781e-02 ... -1.43811047e-01\n",
            "    -1.70254335e-01 -1.81866452e-01]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qh.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilcjyiHp4z6g",
        "outputId": "1e1741e5-37f5-4ddd-ad1b-776589e3d589"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 13, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Understanding the output as a 4d matrix\n",
        "\n",
        "qh = [\n",
        "  [  # batch 0\n",
        "    [  # head 0\n",
        "      [0.1, 0.2, 0.3, 0.4],   # token 0 vector (length4)\n",
        "      [0.5, 0.6, 0.7, 0.8],   # token 1 vector\n",
        "      [0.9, 1.0, 1.1, 1.2],   # token 2 vector\n",
        "    ],\n",
        "    [  # head 1\n",
        "      [1.1, 1.2, 1.3, 1.4],   # token 0 vector\n",
        "      [1.5, 1.6, 1.7, 1.8],   # token 1 vector\n",
        "      [1.9, 2.0, 2.1, 2.2],   # token 2 vector\n",
        "    ]\n",
        "  ]\n",
        "]\n"
      ],
      "metadata": {
        "id": "pqiNxyn2wn59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=None):\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=axis, keepdims=True)\n"
      ],
      "metadata": {
        "id": "sEtlcDHAxtXI"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_scores=np.matmul(qh, kh.transpose(0,1,3,2))/np.sqrt(64)   #dk=dmodel/h\n",
        "\n",
        "attention_weights=softmax(intermediate_scores, axis=-1)\n",
        "context=np.matmul(attention_weights,vh)   ## shape: (1, 12, seq_len, 768/12=64)\n",
        "\n",
        "print(context, context.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kdLlBIaw0P0",
        "outputId": "c6269322-f054-429b-81dc-cdbfde482ccf"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-2.15860419e-01 -2.63770201e-01 -3.53238665e-02 ... -8.88691316e-03\n",
            "     1.40825993e-01  3.43908950e-02]\n",
            "   [-1.52835910e-01  7.10090714e-03  2.68882653e-02 ... -1.84767748e-02\n",
            "     1.91802196e-02  5.47547546e-02]\n",
            "   [-2.19141690e-01 -7.33091592e-02 -4.23792728e-02 ... -4.79674450e-02\n",
            "     6.49535093e-02 -3.89163827e-02]\n",
            "   ...\n",
            "   [-1.39401007e-01  1.64586155e-02 -3.44427815e-02 ... -4.60245643e-02\n",
            "    -2.74281160e-02 -1.67787249e-02]\n",
            "   [-1.63442535e-01 -1.22903116e-02  8.10979404e-03 ...  1.10961323e-02\n",
            "    -3.09907482e-02 -1.49625501e-03]\n",
            "   [-1.46466427e-01 -1.29488365e-02 -4.96935594e-02 ... -7.01406105e-02\n",
            "    -6.34402586e-02  1.26871507e-02]]\n",
            "\n",
            "  [[-8.81407118e-02 -1.90159509e-01 -1.98428137e-02 ...  1.06672875e-01\n",
            "     1.58228752e-01  3.20256440e-01]\n",
            "   [-7.74717636e-02 -1.81456232e-01 -2.24622110e-02 ...  9.24443479e-02\n",
            "     1.44092566e-01  3.07507770e-01]\n",
            "   [-8.95914972e-02 -1.83051561e-01 -1.50604624e-02 ...  1.07630873e-01\n",
            "     1.59088877e-01  3.17300166e-01]\n",
            "   ...\n",
            "   [-8.05452502e-02 -1.84279765e-01 -2.42483066e-02 ...  1.12572490e-01\n",
            "     1.48216341e-01  3.31935190e-01]\n",
            "   [-8.60410501e-02 -1.79044334e-01 -2.68704299e-02 ...  1.24512413e-01\n",
            "     1.54433475e-01  3.30231093e-01]\n",
            "   [-1.71224356e-01 -1.93872734e-01 -1.47704381e-01 ...  2.02350557e-01\n",
            "     1.28233663e-02  4.42675152e-01]]\n",
            "\n",
            "  [[ 7.26671183e-02  2.34843039e-02  3.01892967e-01 ... -3.94616624e-02\n",
            "     1.02531524e-01  7.37311595e-02]\n",
            "   [ 1.24877298e-01  5.80050858e-02  3.76129545e-01 ... -1.09069256e-02\n",
            "     8.54947810e-02  5.46525806e-02]\n",
            "   [ 5.66369810e-02  4.09687239e-03  2.74485346e-01 ... -1.38965988e-03\n",
            "     1.00369629e-01  7.51182906e-02]\n",
            "   ...\n",
            "   [ 1.11163378e-01  2.69948425e-02  3.61399463e-01 ... -4.92508440e-02\n",
            "     1.05212376e-01  4.57551592e-02]\n",
            "   [ 7.29983920e-02  9.83307044e-03  3.65889273e-01 ... -2.69359361e-02\n",
            "     1.01876930e-01  5.51139310e-02]\n",
            "   [ 1.07683930e-01  1.23626154e-01  3.76847412e-01 ...  2.29377902e-02\n",
            "     1.25552253e-01  8.66748017e-02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 4.09557057e-01  1.39806092e-01  6.87608215e-03 ... -1.00809736e-01\n",
            "    -5.11579008e-01  8.02116595e-02]\n",
            "   [ 3.87199016e-01  8.30813751e-02  5.14942906e-02 ... -8.75244662e-02\n",
            "    -4.84121862e-01  8.48403127e-02]\n",
            "   [ 3.68462673e-01  1.01518461e-01  4.59057200e-02 ... -1.20196902e-01\n",
            "    -5.05028577e-01  9.74209565e-02]\n",
            "   ...\n",
            "   [ 3.86917811e-01  9.78744266e-02  4.45944779e-02 ... -9.58866985e-02\n",
            "    -4.90848994e-01  8.90437758e-02]\n",
            "   [ 4.01809602e-01  8.47569656e-02  5.03260792e-02 ... -8.26934091e-02\n",
            "    -4.91994977e-01  7.54996538e-02]\n",
            "   [ 3.57756274e-01  2.16954735e-02  6.82567379e-02 ... -6.20362123e-02\n",
            "    -4.92062888e-01  7.24099160e-02]]\n",
            "\n",
            "  [[-1.44056345e-01 -1.94008101e-01  2.42685968e-02 ...  1.63462877e-01\n",
            "    -1.25063817e-01  3.02604552e-02]\n",
            "   [-1.18609177e-01 -1.34234079e-01  2.71333278e-02 ...  1.55385982e-01\n",
            "    -1.30440614e-01  4.58309781e-02]\n",
            "   [-1.31830984e-01 -1.90899488e-01  7.48006227e-03 ...  1.46759278e-01\n",
            "    -1.28852413e-01  2.95942096e-02]\n",
            "   ...\n",
            "   [-1.33613815e-01 -1.58059682e-01  2.59828303e-02 ...  1.52634296e-01\n",
            "    -1.09925809e-01  4.74573560e-02]\n",
            "   [-1.46713916e-01 -1.92282132e-01  3.67882791e-02 ...  1.39288964e-01\n",
            "    -1.20558325e-01  4.82487109e-02]\n",
            "   [-8.73032922e-02 -1.57903331e-01 -1.11852500e-01 ...  1.16409848e-01\n",
            "    -2.64048899e-01  4.68598792e-02]]\n",
            "\n",
            "  [[ 7.26425730e-02 -1.16657688e-04  1.21508956e-01 ... -6.97817142e-02\n",
            "    -1.30653893e-01 -1.05969913e-01]\n",
            "   [ 6.36513591e-02  8.17333608e-04  1.49928382e-01 ... -6.05298654e-02\n",
            "    -1.39028560e-01 -9.42442516e-02]\n",
            "   [ 7.47082275e-02  1.78555821e-03  8.67784319e-02 ... -1.16566061e-01\n",
            "    -1.32672923e-01 -9.46606997e-02]\n",
            "   ...\n",
            "   [ 5.39245821e-02 -1.19421899e-04  1.49681755e-01 ... -6.76210179e-02\n",
            "    -1.44126582e-01 -9.86257394e-02]\n",
            "   [ 5.95987331e-02 -2.99321623e-03  1.52851463e-01 ... -5.94526683e-02\n",
            "    -1.42861188e-01 -1.04011918e-01]\n",
            "   [ 1.83462255e-01  3.62827343e-03 -6.01819692e-02 ... -2.42398919e-01\n",
            "    -1.38196910e-01 -9.70490482e-02]]]] (1, 12, 13, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"qh shape:\", qh.shape)\n",
        "print(\"kh shape:\", kh.shape)\n",
        "print(\"vh shape:\", vh.shape)\n",
        "print(\"attention_weights shape:\", attention_weights.shape)\n",
        "print(\"context shape:\", context.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9xjUij7w0MO",
        "outputId": "123150c6-8950-4187-f4fd-0dd1adb2d935"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qh shape: (1, 12, 13, 64)\n",
            "kh shape: (1, 12, 13, 64)\n",
            "vh shape: (1, 12, 13, 64)\n",
            "attention_weights shape: (1, 12, 13, 13)\n",
            "context shape: (1, 12, 13, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-b0vETezzOxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate heads\n",
        "context_concat = context.transpose(0, 2, 1, 3).reshape(1, -1, 768)"
      ],
      "metadata": {
        "id": "P1NHqybpycig"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_concat.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpAfAifD6vKH",
        "outputId": "6b49c6d4-87b5-43ce-c8e2-39743b9925f4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 13, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final attention projection\n",
        "attn_output = np.matmul(context_concat, o_w.T)  # (1, seq_len, 768)"
      ],
      "metadata": {
        "id": "O4Gygbb66y9a"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDIjCd4R656T",
        "outputId": "97dd934f-fe9d-4dc0-9b3e-453a0b45f986"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 13, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Residual + LayerNorm before FFN\n",
        "res1 = x_embed + attn_output\n",
        "res1_norm = layer_norm(res1, ln2_weight)\n",
        "print(\"After LayerNorm (FFN):\", res1_norm.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFJ3RBFUE8dr",
        "outputId": "bdda0234-25eb-41d8-bdd1-cd3564490065"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After LayerNorm (FFN): (1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ff_layer = model.encoder.block[0].layer[1].DenseReluDense\n",
        "\n",
        "wi_0 = ff_layer.wi_0.weight.detach().numpy()\n",
        "wi_1 = ff_layer.wi_1.weight.detach().numpy()\n",
        "wo    = ff_layer.wo.weight.detach().numpy()\n"
      ],
      "metadata": {
        "id": "uw-lPwT18X-D"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wi_0.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7XrROfA8bcV",
        "outputId": "17965ca2-a21b-4ef5-c800-80621ebd3a0b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2048, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wo.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hABaoSjtI5Ee",
        "outputId": "564d0468-19c8-4785-f4dc-49a45e568553"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 2048)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FFN(gated)\n",
        "wi0_out = np.matmul(res1_norm, wi_0.T)\n",
        "wi1_out = np.matmul(res1_norm, wi_1.T)\n",
        "relu_out = np.maximum(0, wi1_out)\n",
        "ffn_out = relu_out * wi0_out\n",
        "ffn_proj = np.matmul(ffn_out, wo.T)\n",
        "print(\"FFN output:\", ffn_proj.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrn3HiAfI_fK",
        "outputId": "e41cbcd8-9913-4854-ac93-19a09194b749"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFN output: (1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final encoder layer output\n",
        "encoder_out = res1 + ffn_proj\n",
        "print(\"Final encoder output shape:\", encoder_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYJ80-RoLZBf",
        "outputId": "0954c1a2-b003-4b56-bde6-254798ebe39f"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final encoder output shape: (1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECODER (ASSUMING GENERATED TOKEN IS \"SOS BONJOUR COMMENT\")"
      ],
      "metadata": {
        "id": "FWxGOS79OHQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_text = \"<pad> bonjour comment\"  # assuming <pad> or <s> token as SOS/start\n"
      ],
      "metadata": {
        "id": "DcKuPpvcOJKQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize decoder input text\n",
        "decoder_inputs = tokenizer(decoder_text, return_tensors=\"pt\")\n",
        "decoder_input_ids = decoder_inputs.input_ids  # (1, seq_len)\n",
        "print(decoder_input_ids.shape)\n",
        "print(decoder_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0YU6ZCXPPUI",
        "outputId": "89aaa727-130b-4534-f92f-b2d3fdefda11"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "{'input_ids': tensor([[    0,  2682, 18359,  1670,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder_inputs.input_ids)\n",
        "print([tokenizer.decode([tid]) for tid in decoder_inputs.input_ids[0]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNMM1qwOqhLJ",
        "outputId": "a80c404c-a5ca-468d-8a93-9f0f0ac4331a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,  2682, 18359,  1670,     1]])\n",
            "['<pad>', 'bon', 'jour', 'comment', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get decoder embedding weights from model\n",
        "decoder_embedding_weights = model.decoder.embed_tokens.weight.detach().numpy()  # (vocab_size, d_model)\n",
        "print(decoder_embedding_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6JkixlzPqoA",
        "outputId": "bceb31ec-e79b-4705-9c17-19a603b49383"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32128, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input_ids to numpy\n",
        "decoder_input_ids_np = decoder_input_ids.detach().numpy()"
      ],
      "metadata": {
        "id": "IPMWTqmbQbGk"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings for decoder input tokens\n",
        "dec_embedded = decoder_embedding_weights[decoder_input_ids_np[0]]  # (seq_len, d_model)\n",
        "dec_embedded = dec_embedded[np.newaxis, ...]  # Add batch dim: (1, seq_len, d_model)\n",
        "print(dec_embedded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxs6-OeiQfs4",
        "outputId": "72c38d78-163d-4bbe-b9aa-3ae9bbfee26f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_layer = model.decoder.block[0]\n",
        "\n",
        "# LayerNorm weights\n",
        "dec_ln1_weight = decoder_layer.layer[0].layer_norm.weight.detach().numpy()\n",
        "dec_ln2_weight = decoder_layer.layer[1].layer_norm.weight.detach().numpy()\n",
        "dec_ln3_weight = decoder_layer.layer[2].layer_norm.weight.detach().numpy()\n",
        "\n",
        "\n",
        "# Self-attention weights\n",
        "dec_self_q = decoder_layer.layer[0].SelfAttention.q.weight.detach().numpy()\n",
        "dec_self_k = decoder_layer.layer[0].SelfAttention.k.weight.detach().numpy()\n",
        "dec_self_v = decoder_layer.layer[0].SelfAttention.v.weight.detach().numpy()\n",
        "dec_self_o = decoder_layer.layer[0].SelfAttention.o.weight.detach().numpy()\n",
        "\n",
        "# Cross-attention weights\n",
        "cross_attn_q = decoder_layer.layer[1].EncDecAttention.q.weight.detach().numpy()\n",
        "cross_attn_k = decoder_layer.layer[1].EncDecAttention.k.weight.detach().numpy()\n",
        "cross_attn_v = decoder_layer.layer[1].EncDecAttention.v.weight.detach().numpy()\n",
        "cross_attn_o = decoder_layer.layer[1].EncDecAttention.o.weight.detach().numpy()\n",
        "\n",
        "# Feedforward weights (gated)\n",
        "wi0 = decoder_layer.layer[2].DenseReluDense.wi_0.weight.detach().numpy()\n",
        "wi1 = decoder_layer.layer[2].DenseReluDense.wi_1.weight.detach().numpy()\n",
        "wo = decoder_layer.layer[2].DenseReluDense.wo.weight.detach().numpy()\n"
      ],
      "metadata": {
        "id": "s9GiAdFmQibQ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"LayerNorm weights shapes:\")\n",
        "print(\"dec_ln1_weight:\", dec_ln1_weight.shape)\n",
        "print(\"dec_ln2_weight:\", dec_ln2_weight.shape)\n",
        "print(\"dec_ln3_weight:\", dec_ln3_weight.shape)\n",
        "\n",
        "print(\"\\nSelf-attention weights shapes:\")\n",
        "print(\"dec_self_q:\", dec_self_q.shape)\n",
        "print(\"dec_self_k:\", dec_self_k.shape)\n",
        "print(\"dec_self_v:\", dec_self_v.shape)\n",
        "print(\"dec_self_o:\", dec_self_o.shape)\n",
        "\n",
        "print(\"\\nCross-attention weights shapes:\")\n",
        "print(\"cross_attn_q:\", cross_attn_q.shape)\n",
        "print(\"cross_attn_k:\", cross_attn_k.shape)\n",
        "print(\"cross_attn_v:\", cross_attn_v.shape)\n",
        "print(\"cross_attn_o:\", cross_attn_o.shape)\n",
        "\n",
        "print(\"\\nFeedforward weights (gated) shapes:\")\n",
        "print(\"wi0:\", wi0.shape)\n",
        "print(\"wi1:\", wi1.shape)\n",
        "print(\"wo:\", wo.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmRlt5dgRgA_",
        "outputId": "bc81eaae-7084-4b57-d11b-583274224735"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayerNorm weights shapes:\n",
            "dec_ln1_weight: (768,)\n",
            "dec_ln2_weight: (768,)\n",
            "dec_ln3_weight: (768,)\n",
            "\n",
            "Self-attention weights shapes:\n",
            "dec_self_q: (768, 768)\n",
            "dec_self_k: (768, 768)\n",
            "dec_self_v: (768, 768)\n",
            "dec_self_o: (768, 768)\n",
            "\n",
            "Cross-attention weights shapes:\n",
            "cross_attn_q: (768, 768)\n",
            "cross_attn_k: (768, 768)\n",
            "cross_attn_v: (768, 768)\n",
            "cross_attn_o: (768, 768)\n",
            "\n",
            "Feedforward weights (gated) shapes:\n",
            "wi0: (2048, 768)\n",
            "wi1: (2048, 768)\n",
            "wo: (768, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2Nav91HTQ2H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LayerNorm before self-attention\n",
        "dec_x_norm = layer_norm(dec_embedded, dec_ln1_weight)  # use same layer_norm function you have\n",
        "print(dec_x_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0IiqTWHQw2l",
        "outputId": "5edc8238-45a4-4ef7-eb2e-4b18e5c74c02"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear projections for self-attention\n",
        "Q = np.matmul(dec_x_norm, dec_self_q.T)\n",
        "K = np.matmul(dec_x_norm, dec_self_k.T)\n",
        "V = np.matmul(dec_x_norm, dec_self_v.T)\n",
        "print(Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0ysvfCJQ73H",
        "outputId": "2458985f-5a6e-41e0-ac06-127e3657e887"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q, K, V = [split_heads(x, n_heads) for x in (Q, K, V)]"
      ],
      "metadata": {
        "id": "QlP5ACy4TN5o"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tODiw1MRAkJ",
        "outputId": "9ae4f616-6dc8-4565-8685-a95debe08326"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute scaled dot-product attention scores\n",
        "dk = Q.shape[-1]\n",
        "scores = np.matmul(Q, K.transpose(0,1,3,2)) / np.sqrt(dk)\n",
        "# Mask future tokens (causal mask)\n",
        "seq_len = scores.shape[-1]\n",
        "mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)   #triu->upper triangle k=1=>true(where future tokens should be masked)\n",
        "scores_masked = np.where(mask, -1e9, scores)    #all true is replaced with -1e9\n",
        "\n",
        "print(scores_masked.shape)\n",
        "attn_weights_dec=softmax(scores_masked)\n",
        "print(attn_weights_dec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF21VE0LRvZI",
        "outputId": "2f3aa84e-9676-422e-94af-57fe38de2ac1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 5)\n",
            "(1, 12, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_dec=np.matmul(attn_weights_dec,V)\n",
        "print(attn_output_dec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H05vLaltTEqA",
        "outputId": "06fdbbfd-ddce-4af4-ac6b-8ad396494d12"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_dec = attn_output_dec.transpose(0,2,1,3).reshape(dec_embedded.shape)\n",
        "print(attn_output_dec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEdC_r-OUa4S",
        "outputId": "48810712-4986-4b22-85ac-15b981696046"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output projection\n",
        "attn_out_proj_dec = np.matmul(attn_output_dec, dec_self_o.T)\n",
        "print(attn_out_proj_dec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NWYLWhVVMDl",
        "outputId": "fd79369c-615a-474e-f337-a6cf57207c0f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Residual + LayerNorm after self-attention\n",
        "res1 = dec_embedded + attn_out_proj_dec\n",
        "res1_norm = layer_norm(res1, dec_ln2_weight)\n",
        "print(res1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zkHXw8lVmTY",
        "outputId": "70bb0b50-bed9-4c12-b6fc-241062d75ee7"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CROSS ATTENTION"
      ],
      "metadata": {
        "id": "N3AciR3yV99J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear projections for cross-attention (query from decoder residual norm)\n",
        "Q_cross = np.matmul(res1_norm, cross_attn_q.T)\n",
        "print(Q_cross.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thwhOkA-V6YP",
        "outputId": "f00b92e1-3864-41c4-a055-46ac7af3223b"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Key, value from encoder output (already computed earlier)\n",
        "# encoder_out shape: (batch_size, seq_len_enc, d_model)\n",
        "K_cross = np.matmul(encoder_out, cross_attn_k.T)\n",
        "V_cross = np.matmul(encoder_out, cross_attn_v.T)\n",
        "print(K_cross.shape)\n",
        "print(V_cross.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2C5jxe4WLA1",
        "outputId": "7e94dc3e-500f-47c3-ad40-4029c440798a"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 13, 768)\n",
            "(1, 13, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape for multi-head\n",
        "Q_cross = split_heads(Q_cross, n_heads)\n",
        "K_cross = split_heads(K_cross, n_heads)\n",
        "V_cross = split_heads(V_cross, n_heads)\n",
        "print(Q_cross.shape,K_cross.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZM5R-q-WS2u",
        "outputId": "42cfb46d-ba0d-42f5-ad5e-f68b28713eab"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 64) (1, 12, 13, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention scores and softmax\n",
        "scores_cross = np.matmul(Q_cross, K_cross.transpose(0,1,3,2)) / np.sqrt(dk)\n",
        "print(scores_cross.shape)\n",
        "attn_weights_cross = softmax(scores_cross)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ate8hJuJXpM9",
        "outputId": "9e42ba13-eccd-493d-d0ed-6ac3333c35a9"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention output\n",
        "attn_output_cross = np.matmul(attn_weights_cross, V_cross)\n",
        "print(attn_output_cross.shape)\n",
        "# Reshape back and output projection\n",
        "attn_output_cross = attn_output_cross.transpose(0,2,1,3).reshape(res1_norm.shape)\n",
        "attn_out_proj_cross = np.matmul(attn_output_cross, cross_attn_o.T)\n",
        "print(attn_out_proj_cross.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZkpYOj1X6W6",
        "outputId": "603385c1-21ca-4cc7-95c3-fad51c30a568"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12, 5, 64)\n",
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (1, 12, 5, 64) -> (1, 5, 768)\n",
        "attn_output_merged = attn_output_proj_cross.transpose(0, 2, 1, 3).reshape(1, 5, 768)\n",
        "#Residual + LayerNorm after cross-attention\n",
        "res2 = res1_norm + attn_output_merged\n",
        "res2_norm = layer_norm(res2, dec_ln3_weight)\n",
        "print(res2_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kce95LghYaXX",
        "outputId": "5dd8bc5d-65a1-4ba1-ce4c-33343490494c"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FFN(GATED)\n",
        "wi0_out_dec = np.matmul(res2_norm, wi0.T)\n",
        "print(wi0_out_dec.shape)\n",
        "wi1_out_dec = np.matmul(res2_norm, wi1.T)\n",
        "print(wi1_out_dec.shape)\n",
        "relu_out_dec = np.maximum(0, wi0_out_dec)\n",
        "print(relu_out_dec.shape)\n",
        "ffn_out_dec = relu_out_dec * wi1_out_dec\n",
        "ffn_proj_dec = np.matmul(ffn_out_dec, wo.T)\n",
        "\n",
        "print(ffn_proj_dec.shape)\n",
        "\n",
        "# Final output of decoder layer 0\n",
        "decoder_out = res2_norm + ffn_proj_dec\n",
        "print(\"Decoder output shape:\", decoder_out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgz5rpi2ZNSs",
        "outputId": "1c696c1a-73a3-453f-d24c-32df0157d08e"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5, 2048)\n",
            "(1, 5, 2048)\n",
            "(1, 5, 2048)\n",
            "(1, 5, 768)\n",
            "Decoder output shape: (1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final LayerNorm\n",
        "final_ln_weight = model.decoder.final_layer_norm.weight.detach().numpy()  # (768,)\n",
        "decoder_out_norm = layer_norm(decoder_out, final_ln_weight)\n",
        "print(\"Final normed decoder output:\", decoder_out_norm.shape)  # (1, seq_len, 768)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDIvhqRkawhs",
        "outputId": "7b178cb5-fa56-478b-ab54-1f62a3d98d6a"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final normed decoder output: (1, 5, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Output projection to vocabulary/LINEAR LAYER AFTER DECODER\n",
        "lm_head_weight = model.lm_head.weight.detach().numpy()  # (32128, 768)\n",
        "print(lm_head_weight.shape)\n",
        "logits = np.matmul(decoder_out_norm, lm_head_weight.T)  # (1, seq_len, vocab_size)\n",
        "print(\"Logits shape:\", logits.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zidzq0iZcOkT",
        "outputId": "e713c716-84fd-44e9-930a-bb41c89f6f4b"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32128, 768)\n",
            "Logits shape: (1, 5, 32128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pick the last token's logits and do argmax for predicting next token\n",
        "last_logits = logits[0, -1]  # Shape: (vocab_size,)\n",
        "next_token_id = np.argmax(last_logits)\n",
        "print(\"Predicted token ID:\", next_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThelikAXcrnk",
        "outputId": "78fe379a-43c0-4312-e7cf-5dc16bac6160"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted token ID: 3003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_token = tokenizer.decode([next_token_id])\n",
        "print(\"Predicted next token:\", predicted_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdO-ODPwc7V6",
        "outputId": "3c853c83-0f2f-4bce-9473-dec2ea6caffb"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next token: poli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhpmF1nkc9Gi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}